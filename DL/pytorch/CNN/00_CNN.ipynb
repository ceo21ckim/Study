{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "96cf18f209edf6220e7043c3825950920f4c7ad96ff42ffae85e8b73f5a9541f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from IPython.display import Image\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "- nn : Deep learning을 하기 위해 필요한 모듈이 모아져 있는 패키지 \n",
    "    - e.g. nn.ReLU(), nn.Linear(128, 128)\n",
    "- Functional( F ) : nn과 같은 모듈이 모아져 있지만, 함수의 input으로 반드시 연산되어야 하는 값을 받는다.\n",
    "    - e.g. F.ReLU(x), F.Linear(x, 128, 128)\n",
    "- optim : optimizer들이 들어가 있다. \n",
    "- data_utils: batch generator 등 학습 데이터에 관련된 패키지"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델은 항상 class를 상속 받아야 한다. \n",
    "\n",
    "class Mymodel(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        super(Mymodel, self).__init__() #이 부분을 통해 nn.Module.__init__() 초기치 설정\n",
    "\n",
    "        layer1 = nn.Linear(x_dim, 128)\n",
    "        activation1 = nn.ReLU()\n",
    "        layer2 = nn.Linear(128, y_dim)\n",
    "\n",
    "        self.module = nn.Sequential(\n",
    "            layer1, \n",
    "            activation1,\n",
    "            layer2\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.module(x)\n",
    "        result = F.softmax(out, dim = 1)\n",
    "        return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyloss() # loss function\n",
    "lr = 1e-5 # learning rate \n",
    "optimizer = optim.SGD(model.parameter(), lr = lr)\n",
    "epochs = 2\n",
    "batchs = len(train_loader)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\tx, x_labels = data # x.size() = [batch, channel, x, y]\n",
    "\t\t# init grad\n",
    "\t\toptimizer.zero_grad() # step과 zero_grad는 쌍을 이루는 것이라고 생각하면 됨\n",
    "\t\t# forward\n",
    "\t\tpred = model(x)\n",
    "\t\t# calculate loss\n",
    "\t\tloss = criterion(pred, x_labels)\n",
    "\t\t# backpropagation\n",
    "\t\tloss.backward()\n",
    "\t\t# weight update\n",
    "\t\toptimizer.step()\n",
    "\t\t# 학습과정 출력\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tif (i+1)%2000 == 0: # print every 2000 mini-batches\n",
    "\t\t\tprint(\"epoch: {}/{} | step: {}/{} | loss: {:.4f}\".format(epoch, num_epochs, i+1, num_batches, running_loss/2000))\n",
    "\t\t\trunning_loss = 0.0\n",
    "\n",
    "print(\"finish Training!\")"
   ]
  }
 ]
}