{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Author : KIM DONG EON\n",
    "@Edits : 2022-01-18 17:41:00\n",
    "@LastEditors : KIM DONG EON \n",
    "@LastEdit : \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbeced31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202f722",
   "metadata": {},
   "source": [
    "### torchtext의 경우 대문자를 소문자로 변환하여 출력된다. tokenizer마다 조금씩 다른 것을 볼 수 있다.\n",
    "갖고있는 코퍼스에서 단어를 걸러낼 때, 온점이나 특수문자를 무조건적으로 제외하는 것은 옳지 않은 분석 방법이다. 달러 기호나 슬래시의 경우는 돈을 의미할 수도 있고, 시간을 나타내기 위해서 슬래시를 사용했을 수도 있기 때문이다.\n",
    "\n",
    "추가적으로 줄임말이나 단어 내 띄어쓰기가 있는 경우를 제대로 처리하는 것도 중요한 과제 중 하나라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80678964",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8142e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('word_tokenize', word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4056d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunctTokenizer ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('WordPunctTokenizer', WordPunctTokenizer().tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4991a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchtext ['don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'mr', '.', 'jone', \"'\", 's', 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "print('torchtext', tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14731dfd",
   "metadata": {},
   "source": [
    "### 표준 토큰화 예제\n",
    "TreebankWordTokenizer는 Penn Treebank Tokenization의 규칙으로 토큰화 해주는 모듈이다.\n",
    "- home-based는 한 단어로 취급하고 있으며, doesn't의 경우는 does + n't 로 분리된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c90dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5884ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드 토크나이저 :  ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'bave', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't bave a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드 토크나이저 : ', tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca17a95",
   "metadata": {},
   "source": [
    "### 문장 토큰화(Sentence Tokenization)\n",
    "갖고 있는 코퍼스 내에서 문장 단위로 구분하는 작업을 할 수 있다. 코퍼스가 정제되지 않은 상태라면, 코퍼스를 문장단위로 바꿔주는 작업이 필요하다.\n",
    "- 문장을 (.)이나 (?) 등으로 구분하게 된다면 아래의 경우에는 큰 오류를 범할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1b9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"IP 192.168.56.32 서버에 들어가서 로그 파일 저장해서 aaa@gamil.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\"\n",
    "sentence2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1457cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1b3c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 :  ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him craze.', 'Finally, the barber went up a cliff.', 'He dug a hole in the midst of sme reeds.', 'He looked about, to make sure no one was nears.']\n",
      "---------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him craze. Finally, \\\n",
    "the barber went up a cliff. He dug a hole in the midst of sme reeds. He looked about, to make sure no one was nears.\"\n",
    "\n",
    "print('문장 토큰화1 : ', sent_tokenize(text))\n",
    "print('-' * 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "636be115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IP 192.168.56.32 서버에 들어가서 로그 파일 저장해서 aaa@gamil.com로 결과 좀 보내줘.', '그 후 점심 먹으러 가자.']\n",
      "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(sentence1))\n",
    "print(sent_tokenize(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b6a53dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :', sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf18e7",
   "metadata": {},
   "source": [
    "NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에 위 문장을 제대로 구분할 수 있었다. 한국어의 경우에는 KSS(Korean Sentence Splitter)가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce906c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccf543a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는... 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "text = '딥 러닝 자연어 처리가 재미있기는... 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. \\\n",
    "이제 해보면 알걸요?'\n",
    "\n",
    "print('한국어 문장 토큰화 :', kss.split_sentences(text) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7d1e8",
   "metadata": {},
   "source": [
    "### NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
    "- PRP : 인칭 대명사\n",
    "- VBP : 동사\n",
    "- RB : 부사 \n",
    "- VBG : 현재부사\n",
    "- IN : 전치사\n",
    "- NNP : 고유명사\n",
    "- NNS : 복수형 명사\n",
    "- CC : 접속사\n",
    "- DT : 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a4a617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4b8fee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 :  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 :  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 : ', tokenized_sentence)\n",
    "print('품사 태깅 : ', pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd07fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt \n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afe47ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b46c8",
   "metadata": {},
   "source": [
    "- morphs : 형태소 추출\n",
    "- pos : 품사 태깅(part-of-speech tagging)\n",
    "- nouns : 명사 추출\n",
    "\n",
    "형태소 분석기에 따라 출력하는 값이 조금 차이가 있다. 필요 용도에 따라 어떤 형태소 분석기가 가장 적절한지를 판단하여 사용하면된다. Okt는 SNS와 같은 언어에 좋다고 알려져있고, 속도면에서 빠른 것을 선호하면 Mecab을 사용하는 것을 추천한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b74f9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 :  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요', '.']\n",
      "OKT 품사 태깅 :  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb'), ('.', 'Punctuation')]\n",
      "OKT 명사 추출 :  ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"\n",
    "print('OKT 형태소 분석 : ', okt.morphs(sentence))\n",
    "print('OKT 품사 태깅 : ', okt.pos(sentence))\n",
    "print('OKT 명사 추출 : ', okt.nouns(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "022dfa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKMA 형태소 분석 :  ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요', '.']\n",
      "KKMA 품사 태깅 :  [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN'), ('.', 'SF')]\n",
      "KKMA 명사 추출 :  ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"\n",
    "print('KKMA 형태소 분석 : ', kkma.morphs(sentence))\n",
    "print('KKMA 품사 태깅 : ', kkma.pos(sentence))\n",
    "print('KKMA 명사 추출 : ', kkma.nouns(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d9977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6.3(pytorch)",
   "language": "python",
   "name": "khu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
